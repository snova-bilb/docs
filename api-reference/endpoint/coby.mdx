---
title: "Coby.mdx"
description: "SambaNova Cloud API Reference"
---

## SambaNova Cloud API Reference

# Chat

## Create chat completions

Creates a model response for the given chat conversation.

POST [https://api.sambanovacloud.com/v1/chat/completions](https://api.sambanovacloud.com/v1/chat/completions)


### Request body

#### Reference

ParameterDefinitionTypeValues

model

The name of the model to query.

string

Refer to the Model List.

messages

A list of messages comprising the conversation so far.

array of objects

Array of message objects, each containing:
• **role** (string, required)**:** The role of the message author. Choice between: `system`, `user`, or `assistant`.
• **content** (required)**:** The contents of the message.
  - For text-only messages, provide the content as a simple string (e.g., `"content": "Answer the question in a couple sentences."`).
  - For multimodal content, use an array of objects to represent different content types, such as text and image. Example format for multimodal:
      - `[{ "type": "text", "text": "What's in this image?" }, { "type": "image_url", "image_url": { "url": "base64 encoded string of image" } }]`

max\_tokens

The maximum number of tokens to generate.

integer

The total length of input tokens and generated tokens is limited by the model’s context length. Default value is the context length of the model.

temperature

Determines the degree of randomness in the response.

float

The temperature value can be between 0 and 1.

top\_p

The `top_p` (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities.

float

The value can be between 0 and 1.

top\_k

The `top_k` parameter is used to limit the number of choices for the next predicted word or token.

int

The top k value can be between 1 to 100.

stop

Up to 4 sequences where the API will stop generating further tokens.

string, array or null

Default is null.

stream

If set, partial message deltas will be sent.

boolean or null

Default is false.

stream\_option

Options for streaming response. Only set this when you set `stream: true`.

object or null

Default is null.

**Available Options:**
`include_usage`: boolean

#### Sample Request (Text Model)

Below is a sample request body for a streaming response.

\{
&#x20;  "messages": \[
&#x20;     \{"role": "system", "content": "Answer the question in a couple sentences."},
&#x20;     \{"role": "user", "content": "Share a happy story with me"}
&#x20;  ],
&#x20;  "max\_tokens": 800,
&#x20;  "stop": \["\[INST", "\[INST]", "\[/INST]", "\[/INST]"],
&#x20;  "model": "Meta-Llama-3.1-8B-Instruct",
&#x20;  "stream": true,&#x20;
&#x20;  "stream\_options": \{"include\_usage": true}
}


#### Sample Request (Image)

\{
&#x20;   "model": "Llama-3.2-11B-Vision-Instruct",
&#x20;   "messages": \[
&#x20;     \{
&#x20;       "role": "user",
&#x20;       "content": \[
&#x20;         \{
&#x20;           "type": "text",
&#x20;           "text": "What'\\''s in this image?"
&#x20;         },
&#x20;         \{
&#x20;           "type": "image\_url",
&#x20;           "image\_url": \{
&#x20;             "url": f"data:image/jpeg;base64,\{base64\_image}"
&#x20;           }
&#x20;         },
&#x20;         \{
&#x20;           "type": "text",
&#x20;           "text": "Summarize"
&#x20;         },
&#x20;       ]
&#x20;     }
&#x20;   ],
&#x20;   "max\_tokens": 300, # Optional, default 1000
&#x20;   "temperature": \[0-1], # Optional default 0
&#x20;   "top\_p": \[0-1], # Optional default 0
&#x20;   "top\_k": \[0-100], # Optional default 100
&#x20;   "stop": \["\<eot>"], # Optional
&#x20; }


## Response

The API returns a **chat completion object** , or a streamed sequence of **chat completion chunk** objects, if the request is streamed.

### Chat completion object

Represents a chat completion response returned by model, based on the provided input.

#### Reference

**PropertyTypeDescription**

**id**

`string`

A unique identifier for the chat completion.

**choices**

`array`

A list containing a single chat completion.

**created**

`integer`

The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.

**model**

`string`

The model used to generate the completion.

**object**

`string`

The object type, which is always `chat.completion`.

**usage**

`object`

An optional field present when `stream_options: {"include_usage": true}` is set.
When present, it contains a `null` value except for the last chunk, which contains the token usage statistics for the entire request.

**Values returned are:**
• `throughput_after_first_token`: The rate (as tokens per second) at which output tokens are generated after the first token has been delivered
• `time_to_first_token`: The time (in seconds) model takes to generate the first token
• `model_execution_time`: The time (in seconds) to generate a complete response or all tokens
• `output_tokens_count`: Number of tokens generated in the response.
• `input_tokens_count`: Number of tokens in the input prompt.
• `total_tokens_count`: The sum of input and output tokens.\<br•- `queue_time`: The time (in seconds) a request spends waiting in the queue before being processed by the model.

#### Sample

\{
&#x20; "id": "chatcmpl-123",
&#x20; "object": "chat.completion",
&#x20; "created": 1677652288,
&#x20; "model": "Llama-3-8b-chat",
&#x20; "choices": \[\{
&#x20;   "index": 0,
&#x20;   "message": \{
&#x20;     "role": "assistant",
&#x20;     "content": "\n\nHello there, how may I assist you today?",
&#x20;   },
&#x20;   "logprobs": null,
&#x20;   "finish\_reason": "stop"
&#x20; }]
}


### Chat completion chunk object

Represents a streamed chunk of a chat completion response returned by model, based on the provided input.

#### Reference

**PropertyTypeDescription**

id

`string`

A unique identifier for the chat completion.

choices

`array`

A list containing a single chat completion.

created

`integer`

The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.

model

`string`

The model used to generate the completion.

object

`string`

The object type, which is always `chat.completion`.

usage

`object`

An optional field present when `stream_options: {"include_usage": true}` is set.
When present, it contains a `null` value except for the last chunk, which contains the token usage statistics for the entire request.

**Values returned are:**
• `throughput_after_first_token`: The rate (as tokens per second) at which output tokens are generated after the first token has been delivered.
• `time_to_first_token`: The time (in seconds) the model takes to generate the first token.
• `model_execution_time`: The time (in seconds) to generate a complete response or all tokens.
• `output_tokens_count`: Number of tokens generated in the response.
• `input_tokens_count`: Number of tokens in the input prompt.
• `total_tokens_count`: The sum of input and output tokens.
• `queue_time`: The time (in seconds) a request spends waiting in the queue before being processed by the model.

#### Sample

\{
&#x20; "id": "chatcmpl-123",
&#x20; "object": "chat.completion.chunk",
&#x20; "created": 1694268190,
&#x20; "model": "Llama-3-8b-chat",
&#x20; "system\_fingerprint": "fp\_44709d6fcb",
&#x20; "choices": \[
&#x20;   \{
&#x20;     "index": 0,
&#x20;     "delta": \{},
&#x20;     "logprobs": null,
&#x20;     "finish\_reason": "stop"
&#x20;   }
&#x20; ]
}


### Errors

If a request fails, the response body provides a JSON object with details about the error.
For more information on errors, refer to the [API Error Codes](https://community.sambanova.ai/t/api-error-codes/196) article